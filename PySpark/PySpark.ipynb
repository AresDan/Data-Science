{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7160a89",
   "metadata": {},
   "source": [
    "# PySpark (on GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10fb8b",
   "metadata": {},
   "source": [
    "<b>Create a cluster:</b>\n",
    "\n",
    "<i>gcloud dataproc clusters create CLUSTERNAME --region=REGION</i>\n",
    "\n",
    "\n",
    "<b>To delete:</b>\n",
    "\n",
    "<i>gcloud dataproc clusters delete CLUSTERNAME --region=REGION</i>\n",
    "\n",
    "\n",
    "<b>To list created clusters:</b>\n",
    "\n",
    "<i>gcloud dataproc clusters list --region=REGION</i>\n",
    "\n",
    "And you will be able to see them in \"Compute Engine\" -> \"VM instances\"\n",
    "\n",
    "\n",
    "<b>To connect to cluster, where MASTERNODE is the name of cluster with added \"-m\", and ZONE is a zone where your cluster is in with \"-X\", where X is a specific letter (check \"VM instances\"):</b>\n",
    "\n",
    "<i>gcloud compute ssh MASTERNODE --project=PROJECT --zone=ZONE</i>\n",
    "\n",
    "\n",
    "<b>Start Pyspark session by running:</b>\n",
    "\n",
    "<i>pyspark</i>\n",
    "\n",
    "\n",
    "<b>Submit the job (CLUSTER without \"-m\")</b>\n",
    "\n",
    "<i>gcloud dataproc jobs submit pyspark --cluster=CLUSTER --region=REG ps_script.py -- SCRIPT-ARGS</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52276977",
   "metadata": {},
   "source": [
    "## PySpark code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d09dfd",
   "metadata": {},
   "source": [
    "<b>Read data:</b>\n",
    "\n",
    "<i>data = sc.textFile('')</i>\n",
    "\n",
    "\n",
    "\n",
    "<b>Show the data:</b>\n",
    "\n",
    "<i>data.collect()</i>\n",
    "\n",
    "\n",
    "<b>Show file info:</b>\n",
    "\n",
    "<i>gsutil cat gs://bucket_name/numbers.txt</i>\n",
    "\n",
    "\n",
    "<b>Apply some function:</b>\n",
    "1. Map\n",
    "\n",
    "   ><i>stripped = data.map(lambda line: line.strip())</i>\n",
    "2. Reduce\n",
    "\n",
    "   ><i>intdata.reduce(lambda x,y: x+y)</i>\n",
    "3. Sample(withReplacement, fraction, [seed])\n",
    "\n",
    "   ><i>samp = data.sample(True, 0.5)</i>\n",
    "4. Use predefined function\n",
    "\n",
    "   ><i>from prime import is_prime\n",
    "\n",
    "   >primes = data.filter(is_prime)</i>\n",
    "    \n",
    "5. Show distinct elements\n",
    "\n",
    "   ><i>fields_distinct = fields.distinct()</i>\n",
    "6. Producing a list for each element in the RDD, and then concatenating those lists\n",
    "\n",
    "   ><i>flattened = data.flatMap(lambda line: [x for x in line.split()])</i>\n",
    "7. Counts elements\n",
    "\n",
    "   ><i>uniqwords.count()</i>\n",
    "8. Counts how many times each key appears and ignores their values\n",
    "\n",
    "    >words = data_flat.map(lambda w: (w.lower(), 0))\n",
    "\n",
    "    >words.countByKey()\n",
    "    \n",
    "9. Values for each key are aggregated using the given reduce function\n",
    "\n",
    "    >wordcounts = wordkeys.reduceByKey(lambda x, y: x+y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bffbe9",
   "metadata": {},
   "source": [
    "More you can find here:\n",
    "    \n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n",
    "\n",
    "https://spark.apache.org/docs/0.9.0/scala-programming-guide.html#actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7ec9a",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12cf6b4",
   "metadata": {},
   "source": [
    "#### Given the csv file with format DATE, TMAX, TMIN find and show average of maximum and minimum temperature for a specific year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import sys\n",
    "\n",
    "\n",
    "# Check that input and output files are specified\n",
    "if len(sys.argv) != 3:\n",
    "    print('Usage: ' + sys.argv[0] + ' <in> <out>')\n",
    "    sys.exit(1)\n",
    "inputlocation = sys.argv[1]\n",
    "outputlocation = sys.argv[2]\n",
    "\n",
    "\n",
    "# Configure PySpark variable\n",
    "conf = SparkConf().setAppName('Temperature')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# Actual code\n",
    "data = sc.textFile(inputlocation)\n",
    "\n",
    "# split the data ([YYYY-MM-DD, TMAX, TMIN])\n",
    "data_split = data.map(lambda line: line.split(','))\n",
    "# get the format of [YYYY, TMAX, TMIN, 1] (1 for 1 element)\n",
    "data_year = data_split.map(lambda el: [el[0][:4], [int(el[1]), int(el[2]), 1]])\n",
    "# group by key to have [YYYY, (Iterable <TMAX, TMIN>)]\n",
    "data_iter = data_year.reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1], x[2] + y[2]])\n",
    "# reduce by key and get values\n",
    "data_out = data_iter.map(lambda el: [el[0], [el[1][0]/el[1][2], el[1][1]/el[1][2]]])\n",
    "\n",
    "\n",
    "# Save the output and stop PySpark\n",
    "data_out.saveAsTextFile(outputlocation)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ada42e",
   "metadata": {},
   "source": [
    "#### Given the csv file with format DATE, TMAX, TMIN find and show the date (MM-DD) of a max and min temperature for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import sys\n",
    "\n",
    "\n",
    "# Check that input and output files are specified\n",
    "if len(sys.argv) != 3:\n",
    "    print('Usage: ' + sys.argv[0] + ' <in> <out>')\n",
    "    sys.exit(1)\n",
    "inputlocation = sys.argv[1]\n",
    "outputlocation = sys.argv[2]\n",
    "\n",
    "\n",
    "# Configure PySpark variable\n",
    "conf = SparkConf().setAppName('Temperature')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# Actual code\n",
    "data = sc.textFile(inputlocation)\n",
    "\n",
    "# split the data ([YYYY-MM-DD, TMAX, TMIN])\n",
    "data_split = data.map(lambda line: line.split(','))\n",
    "\n",
    "# separate TMAX and TMIN and later join\n",
    "# get the format of [YYYY, TMAX, day for TMAX] \n",
    "data_max = data_split.map(lambda el: [el[0][:4], [int(el[1]), el[0][5:]]])\n",
    "# get the format of [YYYY, TMAX, day for TMAX] \n",
    "data_min = data_split.map(lambda el: [el[0][:4], [int(el[2]), el[0][5:]]])\n",
    "\n",
    "def larger_element(x, y):\n",
    "    if x[0] > y[0]:\n",
    "        return x\n",
    "    return y\n",
    "\n",
    "def smaller_element(x, y):\n",
    "    if x[0] < y[0]:\n",
    "        return x\n",
    "    return y\n",
    "\n",
    "data_max_reduced = data_max.reduceByKey(larger_element)\n",
    "data_min_reduced = data_min.reduceByKey(smaller_element)\n",
    "\n",
    "# join 2 RDDs\n",
    "data_out = data_max_reduced.join(data_min_reduced)\n",
    "\n",
    "# Save the output and stop PySpark\n",
    "data_out.saveAsTextFile(outputlocation)\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
